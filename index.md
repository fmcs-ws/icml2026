---
layout: default
title: Home
---

# Foundation Models for Computational Science

**ICML 2026 · Seoul, Korea**

## Overview
Foundation models have reshaped machine learning by enabling reusable, transferable representations across tasks. In computational science, however, the term “foundation model” is increasingly used inconsistently, despite the fact that scientific workloads impose distinct constraints: governing PDEs and conservation laws, heterogeneous discretizations and geometries, and stringent requirements for stability, interpretability, and extrapolation under regime shift. Building on recent calls for clarity and rigor (e.g., Choi et al., 2025), this workshop convenes ML researchers, numerical analysts, and computational scientists to formalize definitions and desiderata, compare emerging approaches (e.g., pretrained neural operators, mesh-agnostic surrogates, solver-integrated frameworks), and propose community evaluation standards for reusable foundation models for computational science.

[1] Choi, Youngsoo, Siu Wun Cheung, Youngkyu Kim, Ping-Hsuan Tsai, Alejandro N. Diaz, Ivan Zanardi, Seung Whan Chung et al. "Defining Foundation Models for Computational Science: A Call for Clarity and Rigor." arXiv preprint arXiv:2505.22904 (2025).

## Important Dates
- Paper submission: TBA
- Notification: TBA
- Workshop date: TBA

## Organizers
- Stephen Baek (University of Virginia)
- Johannes Brandstetter (Johannes Kepler University)
- Youngsoo Choi (Lawrence Livermore National Laboratory)
- Eric C. Cyr (Sandia National Laboratories)
- Karthik Duraisamy (University of Michigan)
- Shirley Ho (Flatiron Institute)
- Hyunseok Oh (Gwangju Institute of Science and Technology)
- Diane Oyen (Los Alamos National Laboratory)
